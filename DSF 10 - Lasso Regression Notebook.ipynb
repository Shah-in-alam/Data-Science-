{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "low_memory=False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Introduction & Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen many powerful machine learning methods for regression and classification. However, in the presence of high-dimensional datasets, these models can become prone to overfitting and difficult to interpret. This is where **LASSO Regression** comes in.\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) is a type of analysis typically used in regression models that performs both variable selection and regularization. It introduces a penalty term to the linear regression cost function to shrink some coefficients to exactly zero, effectively removing them from the model. This helps in creating simpler, more interpretable models while improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a dataset with hundreds of features, but only a handful of them are truly relevant for predicting the target variable. Up till now we would plainly throw all features at the model, even ones we could classify as 'noise', potentially leading to a model that is overly complex and performs poorly on unseen data.\n",
    "\n",
    "When we train a normal linear regression model we would get a formula that looks something like this:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x  + \\beta_2 x + \\beta_3 x + \\beta_4 x + \\beta_5 x + ...$$\n",
    "\n",
    "Without a penalty, all features contribute, even if some barely matter. Lasso adds a cost or **penalty** for having large coefficients, so if a certain feature doesn't help much in the final model, Lasso shrinks its coefficient or beta. It uses optimization to find the best coefficients that minimize both the prediction error and the penalty. It doesn’t just subtract a fixed value from the coefficients — rather, it tries to shrink the coefficients in a way that reduces the total cost. If a feature doesn’t improve predictions enough to justify its penalty, its coefficient gets lowerd to 0 which in turn means the feature gets dropped from the dataset.\n",
    "\n",
    "The result is simpler model where some coefficients were lowered, and others where set to 0, effectively dropping the corresponding feature from the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore this by looking at lasso in its simplest form: linear regression. We will dive back into the advertising dataset we used all the way back during our first 'real' class of data science fundamentals. This dataset should not be new, but just to refresh your memory we will quickly recap it. The dataset contains information regarding  sales (in thousands of units) for a particular product as well as advertising budgets (in thousands of dollars) for TV, radio, and newspaper media were issued that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Advertising.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'sales' contains the value we want to predict, and is thus our 'y' column. Knowing this, we can prepare a training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('sales', axis=1), df['sales'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we have a look at lasso regression, we need to take a small step back. We want to see exactly how a lassoo regresion model is an improvement over a linear regression model, but for that we first need to have a trained linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2 Score: 0.8994380241009121\n",
      "Linear Regression MSE: 3.1740973539761015\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = linreg.predict(X_test)\n",
    "\n",
    "print(\"Linear Regression R2 Score:\", r2_score(y_test, y_pred_lr))\n",
    "print(\"Linear Regression MSE:\", mean_squared_error(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s use lasso regression and compare the results. You will notice we are introducing a new parameter called `alpha`. This parameter controls the penalty:\n",
    "\n",
    "- **Higher alpha:** More regularization (shrinks more coefficients to zero).\n",
    "- **Lower alpha:** Less regularization (similar to linear regression).\n",
    "\n",
    "For now, we will start with an alpha of 1. Further down this notebook we will explore how to pick a better value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO Regression R2 Score: 0.9004013443377963\n",
      "LASSO Regression MSE: 3.1436915053679364\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ls = lasso.predict(X_test)\n",
    "\n",
    "print(\"LASSO Regression R2 Score:\", r2_score(y_test, y_pred_ls))\n",
    "print(\"LASSO Regression MSE:\", mean_squared_error(y_test, y_pred_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our R2 and MSE values did indeed change. In this case, they improved slightly! Of course right now this is just dumb luck, as we randomly set our penalty to 1. It is possible that this value is way to high, way to low or close but not quite yet at its optimal spot.\n",
    "\n",
    "There are a few ways we can improve this. The first is a quick manual check. Here we just plot the R2 and MSE for a lot of values of alpha and visually compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: Go over all values of alpha between 0 and 10 with an increment of 0.1. Plot the resulting R2 and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: Do you notice anything peculiar about both plots (or both graphs if you drew them on the same plot)? Can you explain why this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's nice to have a visual overview of the best value for alpha, but it would be even better if we could just run some code and get the best value automatically. Well, it seems like santa came a bit early this year as we actually have some magic code like this!\n",
    "\n",
    "By making use of **Grid Search Cross Validation**, a variant of Cross Validation, we can get a simple and clean answer. First, we need to pass on a 'grid' or a list of alphas, then we also need to define our amount of folds as well as our scoring method. . For each possible value of alpha, it will perform cross validation using the amount of folds provided. The result is a single 'best' alpha based on all attempts by the model which we can use in our lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha from CV: 1.0481131341546852\n"
     ]
    }
   ],
   "source": [
    "alpha_grid = {'alpha': np.logspace(-3, 1, 50)}\n",
    "lasso_cv = GridSearchCV(Lasso(), param_grid=alpha_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Alpha from CV:\", lasso_cv.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3: Compare the best value for alpha you retrieved here with the one you found visually in question one. Is this the same? Is it different? explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: See section 10.3.2\n",
    "##### Question 2: See section 10.3.2\n",
    "##### Question 3: See section 10.3.2\n",
    "##### Question 4: Create a new lasso regression model using the new best alpha. Compare R2 and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5: As said in the beginning, lasso regression isn't just a regularization technique but can also be used for feature selecting. Explain how lasso regression could be used to drop certain columns from the original dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
