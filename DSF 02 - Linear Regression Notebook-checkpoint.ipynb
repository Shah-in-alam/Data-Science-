{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "low_memory=False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns; sns.set()\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction & Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Data Science, one of the most fundamental problems is that of Supervised Learning. In Supervised Learning, we are trying to find a relationship between a dependent variable (y) and one or more independent variables (x). If the fitted model is cool, it can allow us to predict the value of y, given any values for x. \n",
    "\n",
    "In the most simple case, we are dealing with all continuous variables which have a linear relationship. The continuity requirement is especially important for the dependent variable. In essence, the variable should be able to take on any values. A good example of this is temperature or earnings. The second requirement refers to the kind of relationship there exists between the variables. We'll take more about this later. This setup is called the Simple Linear Regression Setup, and will allow us to efficiently fit a Linear Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simple Linear Regression Setup exists when we are trying to relate 2 (or more) variables together. Linear Regression however only allows us to search for linear patterns in data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating of the dataset\n",
    "n_obs = 100\n",
    "x = np.linspace(-3, 3, n_obs)\n",
    "X = x[:, np.newaxis]\n",
    "y = x + x*np.random.normal(2,.5,n_obs)\n",
    "\n",
    "# Plot our data\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we see above is so clear, it is almost too straightforward to directly make some remarks:\n",
    "* If x increases, then y increases\n",
    "* If x decreases, then y decreases\n",
    "* The increase in y is almost always the same\n",
    "* If we know x, then we'll have a pretty good idea of what y will be\n",
    "\n",
    "There is one type of mathematical function which also possesses these properties, and simply by looking at the graph it should be obvious which one this would be. Ask yourself the following question: ``If I could draw a line through the data in order to approximate the data, what would it look like?`` \n",
    "Exactly! Simply a straight line. It is exactly this that we'll be doing when constructing Simple Linear Regression Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we said before, the Linear Regression Model is based around trying to fit a special type of function as best we can within our data. The function we try to fit is called the **Linear function**. This is a function that posesses the properties we demanded in section 1.2. In 2 dimensions like we displayed above, this function is represented as a straight line. The formula of a linear function is as follows:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "The next question we must ask ourselves is how we would like to draw this line. In our figure, we can draw an infinite amound of lines. But which one is the correct one? As we can see in the formula above, there are two constant parameters, $\\beta_0$ and $\\beta_1$. When we change these, we change how our line is drawn. The algorithm we will use to determine the best values for $\\beta_0$ and $\\beta_1$ is called the *Ordinary Least Squares Algorithm*. It comes down to the following: *we minimize the distance between our regression function and each datapoint*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this algorithm, luckily Python gives us a hand. Let's first make a function object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it really is as simple as that. Now we've made our Linear Regression object, let's run the OLS algorithm in order to fit the function on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we've fitted our very first model! \n",
    "\n",
    "Now, of course, simply by saying that our model has been run, doesn't magically allow us to predict values for y. Let's take a look at which function we have produced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept (Beta 0): {}\".format(regressor.intercept_))\n",
    "print(\"Slope (Beta 1): {}\".format(regressor.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to predict values of y!\n",
    "\n",
    "##### Question 1: Predict the values for y when x is -2, pi, and 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to predicting values, it might also be interesting to have a graphical representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph we've produced above, it is abundandly clear that our model is fairly good. However, that might not be convincing for everyone. Next to that, in other cases it might maybe not be so clear. As such, we need certain metrics to assess just how good our model exactly is. \n",
    "\n",
    "Luckily, scikit-learn helps us here as well. There are 2 main criteria for regression evaluation: The $R^2$ score and the $MSE$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Variational Measure ($R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ is fairly straightforward, so we will start with that. The $R^2$ represents the percentage of variance in our response variable (y) that is explained by our model. That means in our case; the percentage of variance in y that can be explained by a variance in x. In other words: if I can see that the value of y has changed (relatively to the mean), how much of that change can I attribute to our model or to the changing of x?\n",
    "The $R^2$ always lies beteen 0 and 1 (between 0 and 100 %). To interpret it, we use the following rules:\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The R2 score of the model on the test set is:', regressor.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: Interpret the R2 score. Is our model any good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $MAE$ is is a little bit more intricate, but also gives us in insight in the predictive abilities of our model. That is; assuming our model can predict values of y based on those of x, how good are those predictions? To asses that, we simply calculate the difference between our predicted values and the observed values. The difference between those 2 values is the error we are making when predicting values. To give us an idea of how good our model actually is we add these values up. \n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "The $MAE$ is the average difference over the predicted and actual target values. It should be evident that the lower this value is, the better the model fits our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The MAE of the model on the test set is:', mean_absolute_error(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $MSE$ is a very similar to the $MAE$, however offers one distinct advantage: it punishes models with large outliers. These outliers often indicate overfitting and can get lost in the $MAE$. Noticing these faults in our model early is important to make correct conclusions.\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum^{n}_{i=1} (\\text{predicted}_i - \\text{true}_i)^2$$\n",
    "\n",
    "Thus, the $MSE$ is the average squared difference over the predicted and actual target values. Going forwards we will mostly be using this metric to evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The MSE of the model on the test set is:', mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: See section 2.3.2\n",
    "##### Question 2: See section 2.4.1\n",
    "##### Question 3: In section 2.3.2 we draw the function line by executing the function at the minimum value of X and the maximum value of X. Adapt the function coefficients and report what happens. What happens if Beta 1 is positive? What happends if it is negative? What happens to the function if Beta 1 is equal to 0?\n",
    "##### Question 4: Think of examples in which linear regression will definitely **not** be the way to go. Give 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
