{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "low_memory=False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introduction & Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we saw how we could use the linear function to accurately draw a line through a cloud of points, and how this might give us predictive value in some situations. In this chapter, we move a little bit further. We're still looking for a function that approximates the relationship we find among variables, but we're no longer working with linear functions. In this case, we're working with *polynomial functions*. This results into **polynomial regression** as a method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we already discussed how the Linear Regression model, though strong, has its limitations. Most importantly, the linear regression model van only model linear relations between variables. It should be noted that this is extremely limiting, and that there are a bunch of relationship variables who might not be linear. (For what exactly a linear relationship is, refer back to the 4 points we defined in chapter 1.)\n",
    "\n",
    "##### Question 1: Can you think of relationships of variables which are definitely not linear? Give 3 examples, and explain why this is the case.\n",
    "\n",
    "To give an example of how such data would look, let's generate a dataset which is definitely not linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 100) \n",
    "rng = np.random.RandomState(42)\n",
    "y = np.sin(4 * x) + x + rng.uniform(size=len(x))\n",
    "X = x[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a scatterplot to illustrate the non-linearity of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: Why is this dataset not linear? Elaborate.\n",
    "\n",
    "##### Question 3: Try to fit a linear regression model on this data. Also calculate the evaluation metrics. Does this model make any sense whatsoever? How would you describe the issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we'll use to fit this type of data is called the **Polynomial Regression Model** and is in essence a generalization of the Linear Regression Model. Where the linear regression model is based upon a linear function, the Polynomial Regression Model is based upon a much broader class of functions in order to include much more complicated function patterns: \n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x^1 + \\beta_2 x^2 + \\beta_3 x^3 + ...$$ \n",
    "\n",
    "You do not need to know the function formula, you should just understand the following things:\n",
    "\n",
    "* Polynomial Regression works with a much broader class of functions than Linear Regression\n",
    "* This allows us to also model non-linear patters\n",
    "* Polynomial Regression requires a \"hyperparameter\"\n",
    "\n",
    "The hyperparameter we refer to, is a number from 0, 1, 2, ... up to as much you wish (though only seldom values above 7 are encountered). This value determines the form of a function. It says how many arches can be found in the function. This number is something we have to give to the model before fitting it.\n",
    "\n",
    "The fitting algorithms then once again takes care of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the previous section, when we want to fit a Polynomial Regression Model, we first have to give the model a certain value for the hyperparameter. Let's start with 0 and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "pol_0 = PolynomialFeatures(degree=0)\n",
    "model.fit(pol_0.fit_transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the code, we've started off from a linear regression model and then \"added\" a polynomial part to it. The interested reader might compare the polynomial function with the linear function and remark why this is.\n",
    "\n",
    "Let's draw the function we have fitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1,1) \n",
    "plt.plot(X_train, y_train, 'o')\n",
    "plt.plot(x, model.predict(pol_0.fit_transform(x))) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's odd!! When our hyperparameter is equal to 0, our function is even worse than the linear regression function... \n",
    "Let's try and see what's going on, shall we?\n",
    "\n",
    "##### Question 4: The code below provides a function combining all the steps in making a polynomial regression (creating the model, fitting the model, and plotting the model) while asking an input for the hyperparameter. Try and figure out which hyperparameter gives the best model. Why is this so? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_test(x):\n",
    "    pol_exp = PolynomialFeatures(degree=x)\n",
    "    model = LinearRegression()\n",
    "    model.fit(pol_exp.fit_transform(X_train), y_train)\n",
    "    print('The R2 score of this model on the train set is:', model.score(pol_exp.fit_transform(X_train), y_train))\n",
    "    x = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1,1) \n",
    "    plt.plot(X_train, y_train, 'o')\n",
    "    plt.plot(x, model.predict(pol_exp.fit_transform(x))) \n",
    "    plt.show()\n",
    "    \n",
    "poly_test(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation criteria we have used for the linear regression model are still valid for polynomial regression. Make the following exercise to see if you can get some information from it:\n",
    "\n",
    "##### Question 5: Calculate both evaluation criteria we have seen for all the models you have tried in question 4. Does it confirm your suspicions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Exercises\n",
    "\n",
    "##### Question 1: See section 3.2\n",
    "\n",
    "##### Question 2: See section 3.2\n",
    "\n",
    "##### Question 3: See section 3.2\n",
    "\n",
    "##### Question 4: See section 3.3.2\n",
    "\n",
    "##### Question 5: See section 3.4\n",
    "\n",
    "##### Question 6: In section 3.3.2 we created a function where wou could pass the factor of our function as a value called x. Finding the best value for x is an important task to get the best model possible. Taking into account what we learned last week, how would you find the ideal value for x to prevent overfitting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
